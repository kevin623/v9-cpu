##论文题目：Improving Network Connection Locality on Multicore Systems


##作者：
Aleksey Pesterev Jacob Strauss† Nickolai Zeldovich Robert T. Morris
MIT CSAIL †Quanta Research Cambridge
falekseyp, nickolai, rtmg@csail.mit.edu jacob.strauss@qrclab.com

##摘要：
对于给定的TCP连接，传入传出数据的处理在不同内核执行：接受中断的内核处理传入的数据包，运行相关代码的内核处理传出的数据。作为结果，访问读写连接状态经常涉及到缓存失效和数据在不同内核缓存间移动，这会占用上百个处理器循环周期，足以显著降低性能。
我们提出了一种称作Affinity-Accept的新的设计，它能让对于一个给定的TCP连接都在同一个内核上进行处理。Affinity-Accept指派网络接口来决定给新连接分配哪个内核。仅仅在CPU调度失衡的情况下才对分配情况进行调整。针对Apache这个网络服务器在48核心的AMD系统上进行测试，Affinity-Accept减少了30%TCP协议栈的时间花费，吞吐量提高了24%。

##主要内容：
众所周知，在多核系统上处理Tcp连接的一个好的策略是把连接划分到多个内核，保证每个连接完全有一个内核处理。它能消除内核中每个连接状态的保护竞争，能消除一个连接被多核处理所引发的缓存一致性问题。运行在不同内核上的活动彼此之间尽可能不发生相互作用，这个原则已经被并行处理广泛采用。
然而，在现实中，即便是相互独立的请求也难以避免不同核心上的业务相互影响。第一，内核通常序列化处理给定tcp端口上的新连接。第二，几乎无法让所有涉及到一个链接的活动只在同一个内核上处理，比如数据包交付，内核级TCP处理，用户进程执行，数据包传输，内存分配等。第三个问题，内核和tcp链接的一对一匹配可能和其它调度策略发生冲突，比如负载平衡。第四，应用程序的设计可能不完全兼容链接的独立处理。
本文提出Affinity-Accept来实现与某一连接相关的所有活动都放到同一内核执行，并给出了在Linux上的实现。第一步，就是以太网控制器，由它根据一个连接表示字段的哈希表为进入的数据包分配接收DMA环，同一个连接的所有数据包对应一个内核。新的tcp连接请求被添加到一个内核队列中并受到该内核加锁保护，这样连接建立就能并行处理。每个服务器进程等待来自自身内核队列的新连接，并且我们假设应用在接受连接请求的内核上处理连接。既然以太网控制器把同一连接的随后传入的数据包放到相同内核的DMA ring，那么最终所有数据包和处理这个连接的应用都会在同一个内核上出现。这样做对连接涉及的共享内存和锁定进行了最小化处理。Affinity-Accept也能平衡在不同时间尺度上用不同处理时间来操纵连接造成的无关计算活动。

一个可替换的解决方案是使用一个智能以太网卡，它利用流表把数据包发到特定内核。内核可以配置网卡按照路径把每个新建tcp连接的数据包送到用户空间的应用上。很不幸，因为受限的表格大小和维持表的开销，导致无法网卡无法工作。
论文在第二部分详细描述了连接管理存在困难。第三部分描述了Affinity-Accept内核设计。第四部分描述了应用层的考虑。第五部分描述实施细节。第六部分测试。第七部分介绍相关工作和结论。

#linux对tcp连接的处理：
这部分简述了linux TCP连接的开始和处理过程，描述并行连接处理的两个难题：套接字锁定和访问共享缓存的昂贵开销。
TCP 监听socket锁
客户端和服务器端通过三次握手建立连接。监听套接字有两个部分追踪握手协议：一个哈希表记录收到SYNs但未收到ACK；一个接收队列记录已经完成三字握手的连接。一个抵达的SYN创建一个实体在哈希表中并触发一个SYN-ACK响应。一个抵达的ACK把连接从哈希表移动到接收队列中。应用程序通过accept（）系统调用使用连接。对于多核系统的传入的tcp连接处理测量很不充分，因为每一个tcp端口的监听套接字有一个哈希表和一个接收队列，它受到一个锁的保护。某个时候只有一个内核能处理传入的syn数据包，ack数据包或者系统调用。
一旦linux建立好了一个Tcp连接，一个应用程序已经接受了它，进一步处理过程测量会变得容易。每个tcp连接锁只允许在某个时间只有一个核心处理连接。如果一个应用有多个活跃的连接，它就允许速购的并行处理。
作者提出的方法在tcp连接建立期间增加了并行处理，区分监听套接字的状态允许更细力度的锁。在第三部分描述这个设计。
在包处理中共享缓存线
linux内核通过每一个建立起的tcp连接联系大量的状态，操纵状态无论何时一个连接的数据包离开或者到来。
在某个应用连接套接字中读写数据的操纵可能是不同内核执行的。其结果是缓存线保持的一个连接的状态在多核之间频繁移动。每次一个内核修改了缓存线，缓存相关硬件一致性必须失效其它这个数据的其它副本。随后在其它内核执行的读数据操作从一个远程的内核缓冲区取数据。这种多核系统上的共享的缓存线非常耗费资源，因为远程访问要比本地访问慢的多。
在多核上处理同一个连接也存在内存分配的性能问题。系统要为传入流量包分配缓存，也要为每个离开内核的数据包指派缓冲。指派接受缓存通过RX DMA ring，取消缓存分配通过调用recvmsg（）。在单核上处理一个连接，不管是分配还是释放都要快速，这是因为它们访问同一个本地缓冲池。而多核由于要远程取消缓冲要慢。

#An Affinity-aware Listen Socket
作者设计了一种新的套接字：Affinity-aware。
第一部分，Affinity-aware利用网卡把传入数据包分散到许多RX DMA环中，通过一种方式确保来自同一数据流的数据包总是指向同一内核。一个天真的方法是通过网卡把应用级别的线程迁移到接受连接数据包的内核，为保障本地处理，线程迁移要消耗很多时间，对于短连接，线程迁移时不划算的。另一个方法是通知网卡把给定数据流的数据包重定向到一个不同的内核。而对于一个服务器在同一时间有成千上百的建好的TCP连接， 网卡的流指导表的容量无法容纳。此外对于短连接来说，为每个数据流重新配置网卡时间消耗很大。

第二部分，Affinity-aware没有采用强制特定tcp连接或者线程到特定内核的方法，而是安排系统调用accept（）优先返回本地连接给运行在本地内核的线程。只要网卡的流量操作没有改变，应用线程没有迁移到另一个内核，所有的tcp连接处理都在本地完成，而无需强行迁移数据流或者线程。
既然类同接受不需强制迁移实现类同连接，它假设网卡能够把负载云横分配到各个内核。可是，内核收到更多或者更少的负载是有原因的。一次，类同接受第三部分设计是动态均衡每个内核负载通过网卡的RX DMA环，抵消中短期的变化。

网卡中的链接路由
类同接受假定多核机器有一个网卡，它使用多个硬件DMA环。每一个内核对应一个RX，一个Tx dma 环把处理传入和传出数据包的负载分散到许多的内核上。多个dma环的优势是当内核访问自己的dma环时，无需进行同步处理。
类同接受利用网卡的数据包路由来实现来自同一连接的传入数据包导向同一内核。网卡对每一个数据包五元组（序列号，源ip，目的ip，源端口，目的端口）哈希处理，利用hash结果查找对应的RX DMA ring。既然来自同一连接的数据包具有相同的哈希值，那么网卡就转发这些来自同一tcp链接的数据包到一个 DMA环。
intel IXGBE网卡支持两种映射哈希值到DMA的机制。第一种称作接收端测量rss。rss利用数据流哈希值来检索128个DMA ring.每一个表中实体有一个4位的标示符来表示dma rings。4位仅仅允许16个dma环。这个局限是IXGBE的特殊，而其他网卡可以路由数据包到所有DMA 环。
第二种机制，IXGBE网卡使用一个流操纵表，称为流流向。FDir把数据流指向64个不同的DMA rings。类同接收使用FDir。哈希表保存在网卡内部，内核可以修改它的值，通过向网卡发送特殊要求。在哈希表中的每一个实体映射一个哈希值到一个6bit的RX DMA环标示符。标的大小收到网卡内存的限制。实际上，这表能够容纳8k到32k流操纵实体。
FDir表只能映射特定数据流到特定的DMA环，并且FDir表没有足够空间保存所有的流的五元组。为了避免这个难题，我们更改哈希函数仅仅利用数据包五元组的一个子集。作者让网卡对源端口的12位进行哈希，总共有4096个值，映射它们到RX DMA 环在不同内核分摊负载。这种方法把内核每次建立新连接都要和网卡通信解放出来，避免每个活跃连接都需要在硬件表中对应一个实体。
3.2 Accepting Local Connections
图1所示，监听套接字锁保护两种数据结构：请求哈希表和接受队列。作者撤除单一的监听套接字锁，对其拆分为多个锁分别保护部分数据结构。接受队列分成每个内核接受队列，用自己的锁来保护。用分开的锁来保护请求哈希表中的每一个。这样做可以避免监听套接字上锁的竞争。在第5部分详细描述。

为实现类同连接，Affinity-Accept修改accept（）这个系统调用的行为。当某个应用调用accept（），Affinity-Accept返回一个连接，该连接来自本内核的接受队列，和监听套接字一致。如果可供使用的本地连接，accept（）休眠。当新的连接抵达，网络独占唤醒任何在本地内核接受队列中的所有线程。这就能实现所欲连接处理在本内核实施。

Connection Load Balancer
可能发生这样事情：一个内核忙于接受简介，而其他内核却在闲置。一个理想系统应该把连接从忙碌的本地接受队列卸载到其他闲置内核。有两个例子可以说明为什么多核能够处理更多的tcp连接。
第一个例子是，一个短期负载受阻于一个内核，可能是因为这个内核正在执行cpu密集操作或者无管的cpu密集操作在运行。为解决短期不平衡，Affinity-Accept执行连接窃取操作。运行在某个内核的应用线程接收来自另一个内核的传入连接。
第二个例子是，长期负载不均衡。Affinity-Accept目标是保持高效的本地处理的连接。为此，Affinity-Accept必须匹配每个内核上的负载。Affinity-Accept实施流组迁移，更改在网卡的FDir表中的流组分配。

Affinity-Accept连接窃取机制由两部分组成：从另外一个内核窃取连接的机制；何时完成窃取以及决定从哪个内核窃取。
窃取机制
当某个窃取者内核决定从一个受害者内核窃取一个连接，它获得一把受害者本地接收队列的锁，将一个连接从队列出列。一旦连接已经被窃取，偷窃者内核执行代码处理这个连接，但是受害者内核仍旧执行传入数据包处理。这样为了解决负载不均衡，短期连接窃取链式违反了类同连接的目标。
偷窃方针:为了解决一个内核应该合适从另一个内核窃取传入连接，Affinity-Accept指明每个内核是忙还是不忙。每个内核根据自己本地接受队列超时长度决定自身忙碌状态。不忙的内核试图从忙碌的内核偷窃连接。忙碌的内核从不从其它内核窃取连接。
每一个不忙的内核使用一个见得启发式方法来选择从远程哪个内核窃取。内核都被排序。每个内核保持一个数用来记录上一次它窃取的内核。查找下一个忙碌的内核。这样不忙的内核可以循环偷窃远程内核，这样达到公平和避免竞争。
Tracking busy cores
每一个内核根据他的接受队列长度决定它的忙碌状态。利用最大本地接受队列长度，Affinity-Accept设置队列长度的高低值。这些值决定何时一个内核标记为忙，何时一个内核的忙状态被清楚。当一个内核的本地接受队列长度超过了高位线，Affinity-Accept标记这个内核状态是忙。既然许多应用接收到连接爆发，接受队列的长度就会显著波动。因此，Affinity-Accept对标记内核不忙是很多谨慎的，它利用一个运行的平均值取代瞬时队列长度。每次一个连接加入到本地队列，就要更新这个加权平均值。我们通过实验确定最大局部接受队列长度的75%和10%作为我们硬件的高低水线。这个数值对于其它硬件可能需要调整。
为了确保发现忙内核不成为自身性能的瓶颈，Affinity-Accept必须允许不忙的内核有效的确定其它内核。为了实现这一点，Affinity-Accept对每一个监听套接字维持一个位向量，每一个核心对应一位用来反映这个核心的忙状态。通过简单的读取，一个核心能够确定哪一个核心是忙的。如果所有核心处于不忙状态，包含位向量的缓冲线在所有内核中保存。如果服务器超载并且所有内核都是忙的，这个缓存线不能读或者更新。
轮询
当一个应用线程调用accept，poll，或者select等待一个传入tcp连接，Affinity-Accept首先扫描本地接收队列。如果没有可用连接，Affinity-Accept在别的内核查找可用连接。如果任何接受队列都没有可用连接，线程就休眠。
当一个新的连接加入到一个接收队列，Affinity-Accept首先唤醒本地等待传入连接的线程。如果没有线程在本地等待，本地内核检查任何不忙状态内核的等待线程，唤醒它们。为了避免失去唤醒机会，等待线程应该首先把自己加入本地接受队列的等待队列，然后检查其它所有内核的接受队列再去休眠。作者没有实现这个方案，依赖超时抓住失去唤醒机会的线程。

Flow Group Migration流群迁移
连接窃取见着了在一个忙状态的用户空间负载，但是它既没有减少处理传入数据包的负载，也没有允许本地连接处理。这样，解决了长时间的负载不均衡。
为了决定何时群组迁移应该出现，Affinity-Accept根据一个内核从其它内核偷窃连接的次数来处理。每隔100毫秒，每一个不忙的内核找到它偷取连接次数最多的受害者，迁移流群从这个受害者内核到自身。在我们的配置中，每100毫秒偷取一二个流群就足够了。忙状态的内核不会迁移额外的流组给自己。

##实现Affinity-Accept 

Affinity-Accept部署在linux2.6.35内核，打了一些补丁，包括TCP监听套接字变化。Affinity-Accept没有创建任何新的系统调用。作者对基础内核增加了1200行代码。通过一个新的内核模块来实现连接负载平衡，代码大约800行。
作者使用2.0.84.9 IXGBE驱动程序。作者没有使用较新的驱动（3.3.9），因为作者遇到了性能退化问题。
作者对驱动进行修改增加了一个模式来配置FDir硬件这样连接负载平衡器能再内核之间迁移连接。作者也增加了一个接口在不同内核之间迁移流群。这些变更需要大约700行代码。
作者修改了Apache HTTP服务器2.2.14版本去禁用一个互斥锁，它用来序列化多个关于调用accept和poll的进程。

#评估
在这部分作者描述了实验装置和工作。接着作者指出，在linux中的监听套接字锁是主要的可扩展瓶颈，作者把一个锁分成许多锁。移除锁瓶颈之后，数据共享成为主要问题，Affinity-Accept通过减少共享进一步改善了吞吐量。客户端使用Affinity-Accept却没有负载均衡器会体验非常糟糕的服务，作者提出了负载均衡器解决了这个不平衡。
作者在运行Apache的48核心的Linux系统进行了测试。采用Affinity-Accept后吞吐量达到原来2.8倍。每个连接在同一个核心运行在tcp协议栈的时间花费上，降低了30%，整体吞吐量改善了24%。








